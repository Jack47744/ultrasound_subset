{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torchnet\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "from ema_pytorch import EMA\n",
    "import wandb\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import xlsxwriter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from networks import * \n",
    "from gan.networks import * \n",
    "from glad_utils import build_dataset, prepare_latents, get_optimizer_img, get_eval_lrs, eval_loop_v2\n",
    "from utils import config, get_dataset, get_default_convnet_setting, get_network, get_time, epoch, evaluate_synset, \\\n",
    "    get_eval_pool, ParamDiffAug, DiffAugment, TensorDataset, match_loss\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import argparse\n",
    "\n",
    "def add_shared_args():\n",
    "    parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "    parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "    parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "    parser.add_argument('--ipc', type=int, default=1, help='image(s) per class')\n",
    "    parser.add_argument('--eval_mode', type=str, default='M',\n",
    "                        help='eval_mode')  # S: the same to training model, M: multi architectures\n",
    "    parser.add_argument('--num_eval', type=int, default=5, help='the number of evaluating randomly initialized models')\n",
    "    parser.add_argument('--eval_it', type=int, default=100, help='how often to evaluate')\n",
    "    parser.add_argument('--save_it', type=int, default=None, help='how often to evaluate')\n",
    "    parser.add_argument('--epoch_eval_train', type=int, default=1000,\n",
    "                        help='epochs to train a model with synthetic data')\n",
    "    parser.add_argument('--Iteration', type=int, default=1000, help='training iterations')\n",
    "\n",
    "    parser.add_argument('--mom_img', type=float, default=0.5, help='momentum for updating synthetic images')\n",
    "\n",
    "    parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "    parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "    parser.add_argument('--batch_test', type=int, default=128, help='batch size for training networks')\n",
    "\n",
    "    parser.add_argument('--pix_init', type=str, default='noise', choices=[\"noise\", \"real\"],\n",
    "                        help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "    parser.add_argument('--dsa', type=str, default='True', choices=['True', 'False'],\n",
    "                        help='whether to use differentiable Siamese augmentation.')\n",
    "    parser.add_argument('--dsa_strategy', type=str, default='color_crop_cutout_flip_scale_rotate',\n",
    "                        help='differentiable Siamese augmentation strategy')\n",
    "    parser.add_argument('--data_path', type=str, default='data', help='dataset path')\n",
    "\n",
    "    parser.add_argument('--save_path', type=str, default='result', help='path to save results')\n",
    "\n",
    "    parser.add_argument('--space', type=str, default='p', choices=['p', 'wp'])\n",
    "    parser.add_argument('--res', type=int, default=128, choices=[128, 256, 512], help='resolution')\n",
    "    parser.add_argument('--layer', type=int, default=12)\n",
    "    parser.add_argument('--avg_w', action='store_true')\n",
    "\n",
    "    parser.add_argument('--eval_all', action='store_true')\n",
    "\n",
    "    parser.add_argument('--min_it', type=bool, default=False)\n",
    "    parser.add_argument('--no_aug', type=bool, default=False)\n",
    "\n",
    "    parser.add_argument('--force_save', action='store_true')\n",
    "\n",
    "    parser.add_argument('--sg_batch', type=int, default=10)\n",
    "\n",
    "    parser.add_argument('--rand_f', action='store_true')\n",
    "\n",
    "    parser.add_argument('--logdir', type=str, default='./logged_files')\n",
    "\n",
    "    parser.add_argument('--wait_eval', action='store_true')\n",
    "\n",
    "    parser.add_argument('--idc_factor', type=int, default=1)\n",
    "\n",
    "    parser.add_argument('--rand_gan_un', action='store_true')\n",
    "    parser.add_argument('--rand_gan_con', action='store_true')\n",
    "\n",
    "    parser.add_argument('--learn_g', action='store_true')\n",
    "\n",
    "    parser.add_argument('--width', type=int, default=128)\n",
    "    parser.add_argument('--depth', type=int, default=5)\n",
    "\n",
    "\n",
    "    parser.add_argument('--special_gan', default=None)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def get_sample_syn_label(labels_all, ratio, min_syn=None, max_syn=None):\n",
    "    _, train_class_counts = np.unique(labels_all.numpy(), return_counts=True)\n",
    "    num_sample_class = [round(ratio*e) for e in train_class_counts]\n",
    "    if max_syn is None:\n",
    "        return np.array(num_sample_class)\n",
    "    else:\n",
    "        return np.clip(np.array(num_sample_class), a_min=min_syn, a_max=max_syn)\n",
    "\n",
    "def get_images(c, n, indices_class, images_all):\n",
    "    if c is not None:\n",
    "        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "        return images_all[idx_shuffle].to(args.device)\n",
    "    else:\n",
    "        assert n > 0, 'n must be larger than 0'\n",
    "        indices_flat = [_ for sublist in indices_class for _ in sublist]\n",
    "        idx_shuffle = np.random.permutation(indices_flat)[:n]\n",
    "        return images_all[idx_shuffle].to(args.device), labels_all[idx_shuffle].to(args.device)\n",
    "\n",
    "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
    "\n",
    "    x = unnormalize(x)\n",
    "    if resize:\n",
    "        if channels is None or w is None or h is None:\n",
    "            print('Number of channels, width and height must be provided for resize.')\n",
    "        x = x.view(x.size(0), channels, w, h)\n",
    "    return x\n",
    "\n",
    "def display_latent(latent, num_classes, n_img, is_denorm=False, channel=1, im_size=(64, 64)):\n",
    "    # reshaped_tensor = latent.view(num_classes*args.ipc, channel, im_size[0], im_size[1])\n",
    "    reshaped_tensor = latent.view(-1, channel, im_size[0], im_size[1])\n",
    "    if is_denorm:\n",
    "        reshaped_tensor = denorm(reshaped_tensor, channel, im_size[0], im_size[1])\n",
    "    reshaped_tensor = reshaped_tensor.view(num_classes, n_img, channel, im_size[0], im_size[1]).detach().cpu()\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(n_img, num_classes, figsize=(num_classes*1.5, n_img))\n",
    "    for i in range(num_classes):\n",
    "        for j in range(n_img):\n",
    "            if args.dataset == \"ultrasound\":\n",
    "                axs[j, i].imshow(reshaped_tensor[i, j].squeeze(0), cmap='gray')\n",
    "            else:\n",
    "                axs[j, i].imshow(reshaped_tensor[i, j].permute(1, 2, 0), )\n",
    "            axs[j, i].axis('off')\n",
    "            axs[j, i].set_title(f'Class {i + 1}, Image {j + 1}', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "label_number_to_name_dict = {\n",
    "    0: '10_3VV', \n",
    "    1: '09_4CH', \n",
    "    2: '04_ABDOMINAL', \n",
    "    3: '13_BACKGROUND', \n",
    "    4: '00_BRAIN-CB', \n",
    "    5: '01_BRAIN-TV', \n",
    "    6: '06_FEMUR',\n",
    "    7: '05_KIDNEYS', \n",
    "    8: '03_LIPS', \n",
    "    9: '12_LVOT', \n",
    "    10: '02_PROFILE', \n",
    "    11: '11_RVOT', \n",
    "    12: '07_SPINE-CORONAL', \n",
    "    13: '08_SPINE-SAGITTAL'\n",
    "}\n",
    "\n",
    "def count_classes(dataloader):\n",
    "    class_counts = Counter()\n",
    "    \n",
    "    for _, labels in tqdm(dataloader):\n",
    "        class_counts.update(labels.cpu().numpy())\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "def get_processed_metrics(metric_list_all):\n",
    "    metric_list_full = []\n",
    "    conf_mtx_list = []\n",
    "\n",
    "    for metric_list in metric_list_all:\n",
    "        metric_dict = metric_list[0]\n",
    "        metric_list_full.append({i:metric_dict[i] for i in metric_dict if i!='confusion_matrix'})\n",
    "        conf_mtx_list.append(metric_dict[\"confusion_matrix\"])\n",
    "\n",
    "    metric_df = pd.DataFrame(metric_list_full)\n",
    "    precision_arr = np.array(metric_df[\"precision\"].tolist())\n",
    "    recall_arr = np.array(metric_df[\"recall\"].tolist())\n",
    "    f1_arr = np.array(metric_df[\"f1\"].tolist())\n",
    "    accuracy_arr = metric_df[\"accuracy\"].to_numpy()\n",
    "    conf_mtx_arr = np.array(conf_mtx_list)\n",
    "\n",
    "    return precision_arr, recall_arr, f1_arr, accuracy_arr, conf_mtx_arr\n",
    "\n",
    "def process_mean_std_metrics(precision_arr, recall_arr, f1_arr, accuracy_arr, conf_mtx_arr, class_count):\n",
    "    mean_df = pd.DataFrame({\n",
    "        \"class\": [label_number_to_name_dict[i] for i in range(num_classes)],\n",
    "        \"precision_avg\": precision_arr.mean(axis=0),\n",
    "        \"recall_avg\": recall_arr.mean(axis=0),\n",
    "        \"f1_avg\": f1_arr.mean(axis=0),\n",
    "        \"img_cnt\": class_count,\n",
    "    }).sort_values(by=['class'])\n",
    "\n",
    "    std_df = pd.DataFrame({\n",
    "        \"class\": [label_number_to_name_dict[i] for i in range(num_classes)],\n",
    "        \"precision_std\": precision_arr.std(axis=0),\n",
    "        \"recall_std\": recall_arr.std(axis=0),\n",
    "        \"f1_std\": f1_arr.std(axis=0),\n",
    "        \"img_cnt\": class_count,\n",
    "    }).sort_values(by=['class'])\n",
    "\n",
    "    mean_conf_mtx = pd.DataFrame(conf_mtx_arr.mean(axis=0))\n",
    "    std_conf_mtx = pd.DataFrame(conf_mtx_arr.std(axis=0))\n",
    "\n",
    "    mean_conf_mtx.rename(columns=label_number_to_name_dict, index=label_number_to_name_dict, inplace=True)\n",
    "    std_conf_mtx.rename(columns=label_number_to_name_dict, index=label_number_to_name_dict, inplace=True)\n",
    "\n",
    "    mean_conf_mtx.sort_index(axis=0, inplace=True)\n",
    "    mean_conf_mtx.sort_index(axis=1, inplace=True)\n",
    "    std_conf_mtx.sort_index(axis=0, inplace=True)\n",
    "    std_conf_mtx.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    mean_acc = accuracy_arr.mean(axis=0)\n",
    "    std_acc = accuracy_arr.std(axis=0)\n",
    "\n",
    "    print(f\"Accuracy avg:{mean_acc:05f}, std:{std_acc:05f}\")\n",
    "\n",
    "    return mean_df, std_df, mean_conf_mtx, std_conf_mtx, mean_acc, std_acc\n",
    "\n",
    "def get_excel_data_range(start_row_arr, start_col_arr, height, width):\n",
    "    start_cell = xlsxwriter.utility.xl_rowcol_to_cell(start_row_arr, start_col_arr)\n",
    "    end_cell = xlsxwriter.utility.xl_rowcol_to_cell(start_row_arr + height - 1, start_col_arr + width - 1)\n",
    "    data_range = f'{start_cell}:{end_cell}'\n",
    "    return data_range\n",
    "\n",
    "def cond_color_cell(worksheet, start_row_arr, start_col_arr, height, width, max_green=True):\n",
    "    data_range = get_excel_data_range(start_row_arr, start_col_arr, height, width)\n",
    "    if max_green:\n",
    "        worksheet.conditional_format(data_range, {'type': '3_color_scale',\n",
    "                                        'min_color': \"#F8696B\",  # Red\n",
    "                                        'mid_color': \"#FFEB84\",  # Yellow\n",
    "                                        'max_color': \"#63BE7B\"})  # Green\n",
    "    else:\n",
    "        worksheet.conditional_format(data_range, {'type': '3_color_scale',\n",
    "                                          'min_color': \"#63BE7B\",  # Green\n",
    "                                          'mid_color': \"#FFEB84\",  # Yellow\n",
    "                                          'max_color': \"#F8696B\"})  # Red\n",
    "        \n",
    "def cond_bar_cell(worksheet, start_row_arr, start_col_arr, height, width, color=\"#63C384\"):\n",
    "    data_range = get_excel_data_range(start_row_arr, start_col_arr, height, width)\n",
    "    worksheet.conditional_format(data_range, {'type': 'data_bar',\n",
    "                                          'bar_color': color})\n",
    "\n",
    "def save_res_dict_excel(res_dict, class_count, dir, file_prefix=\"\"):\n",
    "    for model_eval, metric_test_all in res_dict.items():\n",
    "\n",
    "        precision_arr, recall_arr, f1_arr, accuracy_arr, conf_mtx_arr = get_processed_metrics(metric_test_all)\n",
    "\n",
    "        mean_df, std_df, mean_conf_mtx, std_conf_mtx, mean_acc, std_acc = process_mean_std_metrics(\n",
    "            precision_arr, recall_arr, f1_arr, accuracy_arr, conf_mtx_arr, class_count\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "\n",
    "        file_dir = os.path.join(dir, f\"{file_prefix}{args.dataset}_{args.ipc:03d}_{model_eval}.xlsx\")\n",
    "        with pd.ExcelWriter(file_dir, engine='xlsxwriter') as writer:\n",
    "\n",
    "            workbook  = writer.book\n",
    "            sheet_name = model_eval\n",
    "            worksheet = workbook.add_worksheet(sheet_name)\n",
    "            \n",
    "            row = 2\n",
    "            col = 2\n",
    "            worksheet.write(f'B{row}', 'Mean')\n",
    "            mean_df.to_excel(writer, sheet_name=sheet_name, startrow=row, startcol=col, index=False)\n",
    "            cond_color_cell(worksheet, row+1, col+1, mean_df.shape[0], 1, max_green=True)\n",
    "            cond_color_cell(worksheet, row+1, col+2, mean_df.shape[0], 1, max_green=True)\n",
    "            cond_color_cell(worksheet, row+1, col+3, mean_df.shape[0], 1, max_green=True)\n",
    "            cond_bar_cell(worksheet, row+1, col+4, mean_df.shape[0], 1, color=\"#63C384\")\n",
    "            row += len(mean_df) + 4\n",
    "\n",
    "            worksheet.write(f'B{row}', 'STD')\n",
    "            std_df.to_excel(writer, sheet_name=sheet_name, startrow=row, startcol=col, index=False)\n",
    "            cond_color_cell(worksheet, row+1, col+1, std_df.shape[0], 1, max_green=False)\n",
    "            cond_color_cell(worksheet, row+1, col+2, std_df.shape[0], 1, max_green=False)\n",
    "            cond_color_cell(worksheet, row+1, col+3, std_df.shape[0], 1, max_green=False)\n",
    "            cond_bar_cell(worksheet, row+1, col+4, mean_df.shape[0], 1, color=\"#63C384\")\n",
    "            row += len(std_df) + 4\n",
    "\n",
    "            worksheet.write(f'B{row}', 'Confusion Matrix Mean')\n",
    "            mean_conf_mtx.to_excel(writer, sheet_name=sheet_name, startrow=row, startcol=col, index=True)\n",
    "            cond_color_cell(worksheet, row+1, col+1, mean_conf_mtx.shape[0], mean_conf_mtx.shape[1], max_green=True)\n",
    "            row += len(mean_conf_mtx) + 4\n",
    "\n",
    "            worksheet.write(f'B{row}', 'Confusion Matrix STD')\n",
    "            std_conf_mtx.to_excel(writer, sheet_name=sheet_name, startrow=row, startcol=col, index=True)\n",
    "            cond_color_cell(worksheet, row+1, col+1, std_conf_mtx.shape[0], std_conf_mtx.shape[1], max_green=False)\n",
    "            row += len(std_conf_mtx) + 4\n",
    "\n",
    "            worksheet.write(f'B{row}', 'Accuracy Mean')\n",
    "            worksheet.write(f'C{row}', mean_acc)\n",
    "            row += 1\n",
    "\n",
    "            worksheet.write(f'B{row}', 'Accuracy STD')\n",
    "            worksheet.write(f'C{row}', std_acc)\n",
    "        \n",
    "        print(f\"Save at: {file_dir}\")\n",
    "        \n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "def get_latent_sample(n_sample_list, args, num_classes, im_size=(64, 64)):\n",
    "\n",
    "    n_latent = int(sum(n_sample_list))\n",
    "\n",
    "    label_syn = torch.tensor([i for i in range(num_classes) for _ in range(n_sample_list[i])])\n",
    "    f_latents = None\n",
    "    if args.use_gan:\n",
    "        if args.gan_type == \"dcgan\":\n",
    "            latents = torch.randn(size=(n_latent, args.nz, 1, 1), dtype=torch.float, requires_grad=False, device=args.device)\n",
    "        elif args.gan_type == \"stylegan2\":\n",
    "            latents = torch.rand(size=(n_latent, args.nz), dtype=torch.float, requires_grad=False, device=args.device)\n",
    "    else:\n",
    "        latents = torch.randn(size=(n_latent, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=False, device=args.device)\n",
    "    \n",
    "    latents = latents.detach().to(args.device).requires_grad_(True)\n",
    "\n",
    "    return latents, f_latents, label_syn\n",
    "\n",
    "def get_latent_ipc(args, num_classes):\n",
    "    if args.use_gan:\n",
    "        label_syn = torch.tensor([i*np.ones(args.ipc, dtype=np.int64) for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
    "        latents = torch.randn(size=(num_classes * args.ipc, args.nz, 1, 1), dtype=torch.float, requires_grad=False, device=args.device)\n",
    "        f_latents = None\n",
    "        latents = latents.detach().to(args.device).requires_grad_(True)\n",
    "    else:\n",
    "        latents, f_latents, label_syn = prepare_latents(channel=channel, num_classes=num_classes, im_size=im_size,\n",
    "                                                        zdim=zdim, G=G, class_map_inv=class_map_inv, get_images=get_images,\n",
    "                                                        args=args)\n",
    "        \n",
    "    return latents, f_latents, label_syn\n",
    "\n",
    "def get_latent_sample_class_start_end_idx(c, n_sample_list):\n",
    "    cumsum_n_sample_list = np.cumsum(n_sample_list)\n",
    "    cumsum_n_sample_list = np.append(0, cumsum_n_sample_list)\n",
    "    start = cumsum_n_sample_list[c]\n",
    "    end = cumsum_n_sample_list[c+1]\n",
    "    return start, end\n",
    "\n",
    "def get_latent_sample_class(c, latents, n_sample_list, chunk_size=None, n_chunk=None):\n",
    "    class_start, class_end = get_latent_sample_class_start_end_idx(c, n_sample_list)\n",
    "\n",
    "    if chunk_size is None and n_chunk is None:\n",
    "        return latents[class_start:class_end]\n",
    "    elif class_end - class_start <= chunk_size:\n",
    "        return latents[class_start:class_end]\n",
    "    elif chunk_size is not None and n_chunk is not None:\n",
    "        start_idx = class_start + chunk_size * (n_chunk)\n",
    "        end_idx = start_idx + chunk_size\n",
    "        end_idx = min(end_idx, class_end)\n",
    "        if start_idx >= class_end:\n",
    "            raise IndexError(\"Chunk start index exceeds the boundary of the class segment.\")\n",
    "        return latents[start_idx:end_idx]\n",
    "    else:\n",
    "        raise ValueError(\"Both chunk_size and n_chunk must be specified together or not at all.\")\n",
    "\n",
    "\n",
    "\n",
    "def sample_tensors(tensors, class_counts, n):\n",
    "    \"\"\"\n",
    "    Sample n tensors from each class.\n",
    "\n",
    "    Parameters:\n",
    "    tensors (list): List of tensors.\n",
    "    class_counts (array): Array where each element represents the number of tensors in the corresponding class.\n",
    "    n (int): Number of tensors to sample from each class.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with class indices as keys and lists of sampled tensors as values.\n",
    "    \"\"\"\n",
    "    sampled_tensors = []\n",
    "    start_index = 0\n",
    "    \n",
    "    for class_index, count in enumerate(class_counts):\n",
    "        end_index = start_index + count\n",
    "        class_tensors = tensors[start_index:end_index]\n",
    "        \n",
    "        # Ensure we do not sample more than available tensors\n",
    "        if n > count:\n",
    "            sampled_tensors.append(class_tensors)\n",
    "        else:\n",
    "            sampled_tensors.append(class_tensors[:n])\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    return torch.stack(sampled_tensors)\n",
    "\n",
    "def get_embed_list(args, channel, num_classes, im_size, num_net=10):\n",
    "    net_list = [\n",
    "        get_network(args.model, channel, num_classes, im_size, depth=args.depth, width=args.width).to(args.device).eval() for _ in range(num_net)\n",
    "    ]\n",
    "\n",
    "    embed_list = [net.module.embed if torch.cuda.device_count() > 1 else net.embed for net in net_list]\n",
    "    del net_list\n",
    "\n",
    "    return embed_list\n",
    "\n",
    "def get_most_similar_img(latents_tmp, args, indices_class, images_all, get_mean_embed_only=False, is_stack=True, embed_list=[], ret_img_latent=False):\n",
    "\n",
    "    if not embed_list:\n",
    "        net_list = get_embed_list(args, 10)\n",
    "\n",
    "    mse_latent_dict = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        latent_embed_mean_list = []\n",
    "\n",
    "        img_latent_mean_all_list = []\n",
    "\n",
    "        for c, img_class_idx in enumerate(indices_class):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            latent_embed_class_list = []\n",
    "\n",
    "            img_latent_mean_list = {}\n",
    "            for img_idx in tqdm(img_class_idx):\n",
    "                    \n",
    "                img = images_all[img_idx].to(args.device)\n",
    "                img = torch.unsqueeze(img, 0)\n",
    "\n",
    "                img_embed_list = [embed(img) for embed in embed_list]\n",
    "                img_embed_list = torch.cat(img_embed_list)\n",
    "                img_embed = torch.mean(img_embed_list, 0)\n",
    "                img_latent_mean_list[img_idx] = img_embed\n",
    "            img_latent_mean_all_list.append(img_latent_mean_list)\n",
    "\n",
    "            for latent_idx, latent in enumerate(tqdm(latents_tmp[c], desc=\"Processing Latents\")):\n",
    "                \n",
    "                latent = torch.unsqueeze(latent, 0)\n",
    "                latent_embed_list = [embed(latent) for embed in embed_list]\n",
    "                latent_embed_list = torch.cat(latent_embed_list)\n",
    "                latent_embed = torch.mean(latent_embed_list, 0)\n",
    "                latent_embed_class_list.append(latent_embed)\n",
    "\n",
    "                if get_mean_embed_only:\n",
    "                    continue\n",
    "\n",
    "                for img_idx in img_class_idx:\n",
    "                    img_embed = img_latent_mean_list[img_idx]\n",
    "                    mse_loss = F.mse_loss(img_embed, latent_embed).item()\n",
    "                    mse_latent_dict[(c, latent_idx)].append((mse_loss, img_idx))\n",
    "\n",
    "            latent_embed_class_list = torch.stack(latent_embed_class_list, 0)\n",
    "            latent_embed_mean_list.append(latent_embed_class_list)\n",
    "    if is_stack:\n",
    "        latent_embed_mean_list = torch.stack(latent_embed_mean_list, 0)\n",
    "\n",
    "    if ret_img_latent:\n",
    "        return mse_latent_dict, latent_embed_mean_list, img_latent_mean_all_list\n",
    "    \n",
    "    return mse_latent_dict, latent_embed_mean_list\n",
    "\n",
    "\n",
    "\n",
    "def split_tensor_to_list(tensor, class_counts):\n",
    "    \"\"\"\n",
    "    Split a tensor into a list of tensors based on the provided class counts.\n",
    "\n",
    "    Parameters:\n",
    "    tensor (torch.Tensor): The input tensor of shape (798, 1, 64, 64).\n",
    "    class_counts (array): Array where each element represents the number of tensors in the corresponding class.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tensors where each tensor's number matches the counts in class_counts.\n",
    "    \"\"\"\n",
    "    assert tensor.shape[0] == sum(class_counts), \"The sum of class counts must match the first dimension of the tensor.\"\n",
    "\n",
    "    split_tensors = []\n",
    "    start_index = 0\n",
    "    \n",
    "    for count in class_counts:\n",
    "        end_index = start_index + count\n",
    "        split_tensors.append(tensor[start_index:end_index])\n",
    "        start_index = end_index\n",
    "    \n",
    "    return split_tensors\n",
    "\n",
    "def plot_images_with_similarity(all_img_top_k_list, similarity_loss_list, ipc, num_classes):\n",
    "    # Determine the number of classes\n",
    "\n",
    "    \n",
    "    # Create a figure with subplots in a grid: num_classes rows and ipc+1 columns\n",
    "    fig, axs = plt.subplots(nrows=len(all_img_top_k_list), ncols=all_img_top_k_list[0].shape[0], figsize=(ipc * 2, num_classes * 2))\n",
    "    \n",
    "    for i, (images, similarity_losses) in enumerate(zip(all_img_top_k_list, similarity_loss_list)):\n",
    "        # print(images.shape)\n",
    "        for j in range(images.shape[0]):\n",
    "            ax = axs[i, j]\n",
    "            if args.use_gan:\n",
    "                images[j] = denorm(torch.unsqueeze(images[j], 0), channels=None, w=None ,h=None, resize = False)\n",
    "            else:\n",
    "                images[j] = denorm(torch.unsqueeze(images[j], 0), channels=None, w=None ,h=None, resize = False)\n",
    "\n",
    "            if images[j].shape[0] == 3:\n",
    "                img = images[j].permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "                ax.imshow(img.numpy())\n",
    "            else:\n",
    "\n",
    "                ax.imshow(images[j].squeeze(0), cmap='gray')\n",
    "\n",
    "            \n",
    "            ax.axis('off')  # Turn off axis numbers and ticks\n",
    "            \n",
    "            # Annotate the top image with its similarity loss\n",
    "            if j > 0:  # Skip the first image (latent image)\n",
    "                ax.set_title(f\"Loss: {similarity_losses[j-1]:.2f}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_bool(parser, name, default=False):\n",
    "    group = parser.add_mutually_exclusive_group(required=False)\n",
    "    group.add_argument('--' + name, dest=name, action='store_true')\n",
    "    group.add_argument('--no_' + name, dest=name, action='store_false')\n",
    "    parser.set_defaults(**{name:default})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_optimizer(latents, args):\n",
    "    if args.use_gan:\n",
    "        optimizer_img = torch.optim.SGD([latents], lr=args.lr_w, momentum=0.5)\n",
    "    else:\n",
    "        optimizer_img = torch.optim.SGD([latents], lr=args.lr_img, momentum=0.5)\n",
    "    return optimizer_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_sign_augment(image_syn, label_syn):\n",
    "    half_length = image_syn.shape[2]//2\n",
    "    # import pdb; pdb.set_trace()\n",
    "    a, b, c, d = image_syn[:, :, :half_length, :half_length].clone(), image_syn[:, :, half_length:, :half_length].clone(), image_syn[:, :, :half_length, half_length:].clone(), image_syn[:, :, half_length:, half_length:].clone()\n",
    "    a, b, c, d = F.upsample(a, scale_factor=2, mode='bilinear'), F.upsample(b, scale_factor=2, mode='bilinear'), \\\n",
    "        F.upsample(c, scale_factor=2, mode='bilinear'), F.upsample(d, scale_factor=2, mode='bilinear')\n",
    "    # a, b, c, d = image_syn.clone(), image_syn.clone(), image_syn.clone(), image_syn.clone()\n",
    "    image_syn_augmented = torch.concat([a, b, c, d], dim=0)\n",
    "    label_syn_augmented = label_syn.repeat(4)\n",
    "    return image_syn_augmented, label_syn_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_img(mse_latent_dict):\n",
    "    top_k = 1\n",
    "\n",
    "    print(f\" ----- top k: {top_k} ----- \")\n",
    "    top_image_list = []\n",
    "    top_label_list = []\n",
    "    top_image_indices = []  # List to store the indices of the top images\n",
    "\n",
    "    for (c, latent_idx) in tqdm(mse_latent_dict):\n",
    "        k = (c, latent_idx)\n",
    "        mse_val_list = sorted(mse_latent_dict[k])[:top_k]\n",
    "        top_img_idx = [e[1] for e in mse_val_list]\n",
    "        \n",
    "        # Store the index of the top image (the one with the lowest MSE value)\n",
    "        top_image_indices.append(top_img_idx[0])\n",
    "        \n",
    "        top_imgs = images_all[top_img_idx]\n",
    "        top_image_list.append(top_imgs)\n",
    "        top_label_list += [c] * top_k\n",
    "\n",
    "    return top_image_list, top_label_list, top_image_indices\n",
    "\n",
    "\n",
    "def plot_embedding(img_latent_mean_all_list, mse_latent_dict, args, save_path=\"\"):\n",
    "    top_image_list, top_label_list, top_image_indices = get_top_img(mse_latent_dict)\n",
    "\n",
    "    top_class_dict = defaultdict(list)\n",
    "\n",
    "    for top_img, top_label in zip(top_image_list, top_label_list):\n",
    "        top_class_dict[top_label].append(top_img)\n",
    "\n",
    "    for c, img_latent_mean_dict in enumerate(img_latent_mean_all_list):\n",
    "        img_embed_tensor = torch.stack(list(img_latent_mean_dict.values()), dim=0).cpu()\n",
    "        tsne = TSNE(n_components=2, random_state=0, perplexity=50, max_iter=300)\n",
    "        tsne_results = tsne.fit_transform(img_embed_tensor)\n",
    "\n",
    "        top_indices = [i for i, k in enumerate(img_latent_mean_dict.keys()) if k in top_image_indices]\n",
    "\n",
    "        n_all_img = tsne_results.shape[0]\n",
    "        n_top_img = tsne_results[top_indices].shape[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "        ax = sns.scatterplot(x=tsne_results[:, 0], y=tsne_results[:, 1], label=f'All images: {n_all_img} imgs', alpha=0.5)\n",
    "        ax = sns.scatterplot(x=tsne_results[top_indices, 0], y=tsne_results[top_indices, 1], color='red', label=f'Top images: {n_top_img} imgs')\n",
    "        ax.set_title(f't-SNE of the image embeddings {label_number_to_name_dict[c]} {args.method}')\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.legend()\n",
    "\n",
    "        if save_path:\n",
    "            img_save_name = f\"tsne_class_{c:02d}_{label_number_to_name_dict[c]}_{args.method}.png\"\n",
    "            fig.savefig(os.path.join(save_path, img_save_name))\n",
    "\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
